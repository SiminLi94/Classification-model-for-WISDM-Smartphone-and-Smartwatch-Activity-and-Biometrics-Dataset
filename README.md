# Classification Model for WISDM Smartphone & Smartwatch Activity Dataset

This project develops and evaluates machine learning models for classifying human fitness activities using the **WISDM Smartphone and Smartwatch Activity and Biometrics Dataset**.

---

## Dataset Overview

- Collected from **51 subjects** performing **18 activities** (e.g., walking, jogging, climbing stairs).
- Data sourced from smartphones and smartwatches equipped with **inertial sensors** (accelerometer & gyroscope).
- 4 `.arff` files:
  - Phone Accelerometer
  - Phone Gyroscope
  - Watch Accelerometer
  - Watch Gyroscope  
- Each file contains **91 features**, leading to a **total of 364 features**.

---

## Goal

Build a robust classification model that can **accurately predict human activity** from sensor data.

---

## Models Used

### 1. Random Forest
An ensemble method using multiple decision trees.

** Pros:**
- High accuracy  
- Good feature importance estimates  
- Fast test error estimation  

### 2. Gradient Boosting
Sequentially builds models to fix previous errors.

** Pros:**
- Top-tier predictive power
- Flexible and customizable
- Handles missing data

** Cons:**
- Slow training
- Risk of overfitting without regularization
- Used default hyperparameters for faster testing.

### 3. Decision Tree
A simple and interpretable model.

** Pros:**
- No data scaling required
- Handles missing values well
- Easy to explain

** Cons:**
- Lower accuracy compared to ensemble methods

## Project Pipeline

### 1. Data Preprocessing
- Loaded .arff files and combined all sensor data.
- Converted labels from Aâ€“S to integers 1â€“18.
- Merged features by timestamp.
- Shuffled the dataset.

### 2. Hyperparameter Tuning
Used GridSearchCV from sklearn:

**Random Forest:**
- Best: `n_estimators=1000, max_depth=30, criterion='entropy'` â†’ Accuracy: 0.884
- Chosen (faster): `n_estimators=100, max_depth=20, criterion='entropy'` â†’ Accuracy: 0.882

**Decision Tree:**
- Tuned: `max_depth=20, criterion='entropy'`

**Gradient Boosting:**
- Used default settings due to training time.

### 3. Feature Selection
- Total features: 364
- Feature importance visualized with correlation heatmaps.
- Used:
  - SelectFromModel (with Random Forest, threshold=0.005)
  - PCA to reduce to 50 components

### 4. Model Training
- Train/test split: 70/30
- Evaluation metric: accuracy
- Also tested VotingClassifier combining all three models.

## Evaluation Results

### âž¤ Using All 364 Features
| Model             | Accuracy |
|-------------------|----------|
| Random Forest     | 0.8827   |
| Decision Tree     | 0.7656   |
| Gradient Boosting | 0.8803   |

### âž¤ Using 55 Features (SelectFromModel)
| Model             | Accuracy |
|-------------------|----------|
| Random Forest     | 0.8314   |
| Decision Tree     | 0.7325   |
| Gradient Boosting | 0.8010   |
| Voting Classifier | 0.8290   |

### âž¤ Using 55 Features (PCA)
| Model             | Accuracy |
|-------------------|----------|
| Random Forest     | 0.8954 âœ… |
| Decision Tree     | 0.6748   |
| Gradient Boosting | 0.8142   |
| Voting Classifier | 0.8606   |

## Final Verdict
ðŸŽ¯ **PCA + Random Forest** achieved the highest accuracy of 0.8954 and is selected as the final model due to its superior performance and efficiency in high-dimensional space.

## Project Structure
```bash
â”œâ”€â”€ data/
â”‚   â””â”€â”€ *.arff               # Raw sensor data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest.py
â”‚   â”œâ”€â”€ gradient_boosting.py
â”‚   â””â”€â”€ decision_tree.py
â”œâ”€â”€ feature_selection/
â”‚   â”œâ”€â”€ pca.py
â”‚   â””â”€â”€ select_from_model.py
â”œâ”€â”€ train.py                 # Main training pipeline
â”œâ”€â”€ evaluate.py              # Evaluation logic
â”œâ”€â”€ README.md

